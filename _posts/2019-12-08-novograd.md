---
layout: post
title: Stochastic Gradient Methods with Layerwise Adaptive Moments for training of deep networks
categories: [Deep Learning, Optimizers, Theoretical]
tags: [Nvidia, NeurIPS]
---
Novograd, by Ginsburg et al. from Nvidia is a new optimizer from Nvidia accepted at NeurIPS 2019, Vancouver, Canada. The authors propose a new optimizer, NovoGrad based on adaptive SGD that combines: i) Layer wise 2nd moments, ii) GradNorm of 2nd moments and iii) decoupled weight decay. 

Link: [Arxiv](https://arxiv.org/pdf/1905.11286)
<!--end_excerpt-->

## Summary

## Review

## Information
__citation__: `@article{ginsburg2019stochastic,
  title={Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks},
  author={Ginsburg, Boris and Castonguay, Patrice and Hrinchuk, Oleksii and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Nguyen, Huyen and Cohen, Jonathan M},
  journal={arXiv preprint arXiv:1905.11286},
  year={2019}
}`

