---
layout: post
title: AMC-AutoML for Model Compression and Acceleration of Mobile Devices
categories: [AutoML]
tags: [Han Lab, ECCV]
---

AMC, by He and Lin et al., is an AutoML based technique for Model compression using uniform pruning, that uses policy gradient
based reinforcement learning algorithm that decides the compression policy of a pre-trained model.

Link: [Arxiv](https://arxiv.org/pdf/1802.03494.pdf)
<!--end_excerpt-->

## Summary ##
Historically, model compression is a extensively researched topic amongst hardware-machine learning researchers. But, since the inception
of this research, all the research works have faced the same drawback - too large a design space that even domain-experts simply can't connect the dots
when optimizing accuracy, size and latency. Recent works in NAS by Zoph et. al[1] clearly show that neural networks are better suited for designing neural networks 
than any human, which serves as the baseline idea of this research work. 

He and Lin et. al show that given any pre-trained neural network, they're able to achieve state-of-the-art model compression in any unconstrained/metric constrained
environment with __minimal__ to __almost no decrease__ in acuracy in __an hour__ on a Titan X GPU. They prove their results on VGG, ResNet, MobileNet v1 and v2 while exploring
and experimenting with constaint on no. of parameters, FLOPs/MACs, and latency proving the ubiquity of this method. 

### Proposal
![Overview of AMC](/assets/images/AMC_algo_overview.png)

As Eisken et. al[2] defines the components that form a NAS algorithm, the AMC algorithm can broadly be presented in 3 parts:

1. __Compression strategy__ Pruning strategies are employed by AMC to compress the models. Two types of pruning are explored in this work: 
* _Fine-grained_ : Prunes unimportant values in the weights/weights with the least magniture i.e weights below a certain threshold is removed(Han et al.[3], [4]). Here, sparsity ratio of a layer is defined as $#dfrac{#zeros}{(n*c*k*h)}$ . Maximum sparsity ratio is set to 0.8 for Conv layers and 0.98 for dense layers during the experiments.
* _Coarse-grained_: Prunes across an entire dimension(s): for example an entire channel, row, column, etc. Here, sparsity ratio is defined as c_ / c, where c_ is the reduced number of channels after channel-wise pruning(only channels are pruned in coarse-grained based pruning throught the experiment). Maximum sparsity ratio is set to 0.8 for all convolution layers.

In both the pruning techniques, a _dynamic programming like_ strategy is used for deciding the sparsity ratio and weight updates, although the paper only details the algorithm for fine-grained based pruning. The code for channel based pruning can be found [here](https://github.com/mit-han-lab/amc/blob/master/env/channel_pruning_env.py) in their [official repository](https://github.com/mit-han-lab/amc). 

2. __Policy gradient RL__ DDPG(Deep Deterministic Policy Gradient) agent is employed (see the above figure from the paper). 
* _State space_: Embedding vector for each layer composed of (t, n, h, w, stride, k, $FLOPs_{t}$ ,  $FLOPs_{0..t-1}$ , $FLOPS_{t+1..T}$ , $a_{t-1}$ ) where t is the layer index, kernel dims n*c*k*k , input dims c*h*w, $FLOPs_{t}$ is the FLOPs count of layer t, $FLOPS_{a..b}$ FLOPs count between layers a and b, and $a_{t-1}$ is the previous action taken.
* _Action space_: Action space is continuous by definition a $\epsilon$ (0, 1], where the value of $a_{t}$ gives the sparsity ratio of layer t.
* _Agent_ : DDPG agent is an off-policy actor-critic based RL algorithm. In AMC's experiments, noise sampled from a truncated normal distribution is added during exploration and exponentially decayed during exploitation. 

3. __Reward function__ Based on the constraint, the reward function is given as follows:
\begin{equation}
R_{metric} = -Error * log(metric)
\end{equation}
* _Resource-contrained compression_ doesn't use a constraint i.e $R_{err} = -Error$ , instead, limits the action space/sparsity ration of each layer. For instance, in fine-grained pruning, _a_ is allowed to take any value for the first few layers, and later limits _a_ for the next layers, hence allowing an agent to achieve a target compression ratio.
* _Accuracy-guarenteed compression_ uses FLOPs, #params and latency in the reward function as defined earlier, the highlight being the reward is sensitive to both the accuracy and the target metric, but increase in accuracy influences the reward higher than decrease in the metric. This also shows that any type of metric: __proxy__(FLOPs, #params) and __real-time__(latency) can be plugged in.

The paper goes on to provide clear experiments results on CIFAR-10 and a subset of ImageNet which clearly explain the performance boost over all previous research works. They also show latency boost on mobile devices. Some key points in the experiments were that for fine-grained pruning on ImageNet, 4-iterations of pruning and fine-tuning is used with overall density set to [0.5, 0.35, 0.25, 0.20] for each iteration. The RL agent also learns the redundancy of the the 3x3 convolutions encompasses morethan 1x1 convoutions.

## Review ##
A clear paper, with a novel idea and excellent contribution. The foundation, objectives and proposal of the paper were well defined and presented, with ample results to support the claims. 

A minor improvement to achieving a near-perfect paper in my opinion would be to compare their ideas against each other in the results section. While the baseline comparisions are crystal clear, since they've experimented with 2 pruning techniques across different constraints, a table on comparison of these methods against each other on various theoretical and practical metrics could've given a perfect ending.

## Related information ##
__Original paper citation__ 
```
@inproceedings{he2018amc,
  title={AMC: AutoML for Model Compression and Acceleration on Mobile Devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2018}
}
```

## References ##
[1] Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

[2] Elsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: A survey. arXiv preprint arXiv:1808.05377.

[3] Han, S., Mao, H., & Dally, W. J. (2015). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.

[4] Han, S., Pool, J., Tran, J., & Dally, W. (2015). Learning both weights and connections for efficient neural network. In Advances in neural information processing systems (pp. 1135-1143).




