---
layout: post
title: ZeroQ - A Novel Zero Shot Quantization Framework
categories: [Model quantization]
tags: [Peking University, UC Berkeley]
---
## Summary
This paper proposes a Zero shot quantization framework using BatchNorm statistics of hte model. Main contribution of this paper
includes using Distilled Dataset in performing the quantization, that removes the requirement of the training dataset and 
Pareto frontier based mixed-precision quantization.

Link: [Arxiv](https://arxiv.org/pdf/2001.00281)
<!--end_excerpt-->

ZeroQ framework is a Zero shot quantization framework that works sequentially:

1. Generate distilled dataset: using BatchNorm layers(mean, std deviation), synthetic data is generated.
2. Using this dataset, sensitivity analysis is performed layer wise
3. Quantization(uniform and mixed-precision) is performed based on the sensitivity.

Explanation:

In the sequence laid out above:

1. Distilled data
    - Problems of having no dataset: i) Missing range of activations of each layer(to determine clipping range),
    ii) Problem measuring KL divergence between the original model and the quantized model
    - Solution: Using the mean and standard deviation from the BatchNorm layers, train the data to fit the batchnorm statistics
    i.e Learning to generate data 
    \begin{equation}
    L = Layers, N epochs \\
    Init. x^r = Random Gaussian data \\ 
    for i \in [1,L]: \\
        M.forward(x^r) \\
        loss=min_{x^r} \Sigma_{i=0}^{L} ||\mu_{}i^{'r} - \mu_{}i^{r}||^{2} + \sigma_{}i^{'r} - \sigma_{}i^{r}||^{2} \\
          \forall \mu_i , \sigma_i \in BatchNorm_i \\
        x^r = opt.back(loss) \\
    \end{equation}
2. Sensitivity
    - As mentioned earlier, KL Divergence is used to measure sensitivity of each layer \\( i \in [1,L] \\) where 
    \begin{equation}
      \Omega_{i}(k) = \frac{1}{N} \Sigma_{j=1}^{N_{dist}} KL(M(\theta;x_{j}), M(\theta^{'};x_{j}))
    \end{equation}
    where ' indicates quantized model, i indicates layer, k is no. of bits model is quantized to(low \\( \Omega \\) -> less sensitive)
    
3. Pareto-frontier
    - For a mixed-precision setting, sensitivity determines quantization precision - high sensitivity -> higher precision.
    - For a given \\( S_{target} \\), assuming sensitivity of each layer is independent of other layers, calculate bit precision configuration 
    to fit \\( S_{target} \\) and use Dynamic programming to find the best precision setting to minimize overall sensitivity.
    - This is a pareto frontier approach since the possible set of configurations look like the Pareto distribution as mentioned in the paper.(See Fig.1)
    
The paper gives ample experimental evidence and ablation study to support the hypothesis. This approach is quite unique in many differnet ways, 
including it's flexibility, portability, zero-shot nature and speed. The authors' proposal is novel indeed, giving an alternative approach to the learned quantization,
the previous state-of-the-art using RL.

[Official code](https://github.com/amirgholami/ZeroQ)

## Related information ##
__Original paper citation__ 
```
@article{cai2020zeroq,
  title={ZeroQ: A Novel Zero Shot Quantization Framework},
  author={Cai, Yaohui and Yao, Zhewei and Dong, Zhen and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2001.00281},
  year={2020}
}
```
