---
layout: post
title: Automatic differentiation in machine learning - A Survey 
categories: [Automatic Differentiation(AD)]
tags: [Survey]
---

Differentiation and derivatives are a core foundation of machine learning, especially deep learning. Derivatives in the form of gradients and Hessians find their ubiquity everywhere. This paper surveys techniques in AD or also called as algorithmic differentiation. Popular frameworks like PyTorch, Tensorflow, MXNet and recently, Flux and Swift for Tensorflow have a critical AD component in their core definitions that enables researchers and engineers a lot of flexibility.   

Link: [Arxiv](https://arxiv.org/pdf/1502.05767.pdf)
<!--end_excerpt-->
## Summary
Computation of derivaties using computer programs can be classified into 4 categories: 1) Manually working out derivatives and coding them; 2) Numerical differentitation using finite difference approximations; 3) Symbolic differnetiation using exression manipulation in computer algebra systems; 4) Autmatic differnetiation/algorithmic differentiation. Numerical differentitation is often inaccurate due to rounding and truncation errors and more importantly, scales poorly for gradients which is used a lot in ML. Symbolic differentiation addresses the drawbacks of both manual and numerical differentitation, but suffers with _expression swell_ due to complex expressions, a phenomenon where intermediate calculations get extremely complicated to obtain a relatively simple answer in the end. This brings us to automatic differentitation:

Automatic differnentiation(AD) is a special family of techniques that compute derivates during code execution to generate numerical derivative evaluations rather than the derivative expressions. This allows accurate evaluation of derivates at machine precision with only a small constant factor of overhead and ideal asymptotic efficiency. In the ML community, AD's counterpart is termed as back-propogation. 

## Notes
