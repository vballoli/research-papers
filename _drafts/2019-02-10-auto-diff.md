---
layout: post
title: Automatic differentiation in machine learning - A Survey 
categories: [Automatic Differentiation(AD)]
tags: [Survey]
---

Differentiation and derivatives are a core foundation of machine learning, especially deep learning. Derivatives in the form of gradients and Hessians find their ubiquity everywhere. This paper surveys techniques in AD or also called as algorithmic differentiation. Popular frameworks like PyTorch, Tensorflow, MXNet and recently, Flux and Swift for Tensorflow have a critical AD component in their core definitions that enables researchers and engineers a lot of flexibility.   

Link: [Arxiv](https://arxiv.org/pdf/1502.05767.pdf)
<!--end_excerpt-->
## Summary
Computation of derivaties using computer programs can be classified into 4 categories: 1) Manually working out derivatives and coding them; 2) Numerical differentitation using finite difference approximations; 3) Symbolic differnetiation using exression manipulation in computer algebra systems; 4) Autmatic differnetiation/algorithmic differentiation. Numerical differentitation is often inaccurate  

## Notes
